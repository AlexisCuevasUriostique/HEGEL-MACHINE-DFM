PROOF OF CONCEPT, FOR THE HEGEL MACHINE .
ARCHITECTURE: DIALECTICAL FEDERATION MODEL
EXPERIMENT CONCEPT: SOCIETY OF LITTLE DUDES

CODE FOR THE SOCIETY OF LITTTLE DUDES

PHASE 1:
# Phase 1: Two-Agent Peer Parallax Training â€” compact, CPU-only
# This script demonstrates the foundational concept: two agents with different
# "worldviews" (data skews) can improve their robustness by learning from
# each other's specific failures ("negation artifacts").
import random
from typing import List
import torch, torch.nn as nn, torch.nn.functional as F

random.seed(7); torch.manual_seed(7)

TOKENS = ["A","B","C","and","or","implies","not","maybe","ought","harm","safe"]
PAD = "<pad>"; VOCAB = {PAD:0}
for t in TOKENS: VOCAB[t] = len(VOCAB)

def tok(xs: List[str], L=6):
    ids = [VOCAB[x] for x in xs][:L]
    return torch.tensor(ids + [0]*(L-len(ids)), dtype=torch.long)

def gen_valid():
    return random.choice([(["A","and","B"],"YES"),(["A","or","B"],"YES"),(["A","implies","B"],"YES"),
                          (["B","implies","A"],"NO"),(["A","and","C"],"NO"),(["C","or","C"],"YES")])
def gen_invalid():
    return random.choice([(["B","and","C"],"NO"),(["C","implies","B"],"NO"),(["B","or","B"],"YES")])
def gen_contradict():
    x=random.choice(["A","B","C"]); return ([x,"and","not",x],"CONTRADICT")
def gen_ambiguous():
    x,y=random.choice(["A","B","C"]),random.choice(["A","B","C"]); return ([x,"maybe","implies",y],"AMBIGUOUS")
def gen_ethical():
    x=random.choice(["A","B","C"]); return ([x,"implies","harm"],"ETHICAL")

def build_dataset(skew: str, n_total=300):
    props = {
        "contra_heavy": dict(valid=0.36, invalid=0.18, contra=0.20, amb=0.16, eth=0.10),
        "amb_eth_heavy": dict(valid=0.36, invalid=0.18, contra=0.10, amb=0.22, eth=0.14),
    }[skew]
    cnt = {k:max(1,int(v*n_total)) for k,v in props.items()}
    X,Y,T=[],[],[]
    for _ in range(cnt["valid"]): p,l=gen_valid(); X.append(tok(p)); Y.append(l); T.append(0)
    for _ in range(cnt["invalid"]): p,l=gen_invalid(); X.append(tok(p)); Y.append(l); T.append(1)
    for _ in range(cnt["contra"]): p,l=gen_contradict(); X.append(tok(p)); Y.append(l); T.append(2)
    for _ in range(cnt["amb"]): p,l=gen_ambiguous(); X.append(tok(p)); Y.append(l); T.append(3)
    for _ in range(cnt["eth"]): p,l=gen_ethical(); X.append(tok(p)); Y.append(l); T.append(4)
    idx=list(range(len(X))); random.shuffle(idx)
    X=torch.stack([X[i] for i in idx]); Y=[Y[i] for i in idx]; T=torch.tensor([T[i] for i in idx])
    s=int(0.75*len(X))
    return (X[:s],Y[:s],T[:s]), (X[s:],Y[s:],T[s:])

LAB0={"YES":0,"NO":1}
LAB1={"YES":0,"NO":1,"REFUSE":2}
LAB2={"YES":0,"NO":1,"REFUSE":2,"UNCERTAIN":3}
LAB3={"YES":0,"NO":1,"REFUSE":2,"UNCERTAIN":3,"INADVISABLE":4}
SPECIALS={"REFUSE":2,"UNCERTAIN":3,"INADVISABLE":4}

def map_labels(Y, T, phase):
    out=[]
    for y,t in zip(Y,T.tolist()):
        if phase==0: out.append(LAB0[y] if y in ("YES","NO") else LAB0["NO"])
        elif phase==1:
            if y=="CONTRADICT": out.append(LAB1["REFUSE"])
            elif y in ("YES","NO"): out.append(LAB1[y])
            else: out.append(LAB1["NO"])
        elif phase==2:
            if y=="CONTRADICT": out.append(LAB2["REFUSE"])
            elif y=="AMBIGUOUS": out.append(LAB2["UNCERTAIN"])
            elif y in ("YES","NO"): out.append(LAB2[y])
            else: out.append(LAB2["NO"])
        elif phase==3:
            if y=="CONTRADICT": out.append(LAB3["REFUSE"])
            elif y=="AMBIGUOUS": out.append(LAB3["UNCERTAIN"])
            elif y=="ETHICAL": out.append(LAB3["INADVISABLE"])
            elif y in ("YES","NO"): out.append(LAB3[y])
            else: out.append(LAB3["NO"])
    return torch.tensor(out, dtype=torch.long)

class MeanEmbed(nn.Module):
    def __init__(self, vocab, d, n):
        super().__init__(); self.emb=nn.Embedding(len(vocab), d); self.cls=nn.Linear(d,n); self.rat=nn.Linear(d,3)
    def forward(self,x):
        h=self.emb(x).mean(dim=1); return self.cls(h), self.rat(h)

def expand(m,newn):
    old=m.cls; new=nn.Linear(old.in_features,newn)
    with torch.no_grad():
        new.weight[:old.out_features].copy_(old.weight); new.bias[:old.out_features].copy_(old.bias)
        nn.init.zeros_(new.weight[old.out_features:]); nn.init.constant_(new.bias[old.out_features:], -0.1)
    m.cls=new; return m

def train_epoch(m,opt,X,y,train=True,aux=None):
    m.train(train); N=len(X); order=list(range(N))
    if train: random.shuffle(order)
    for i in range(0,N,64):
        idx=order[i:i+64]; lg,rg=m(X[idx]); loss=F.cross_entropy(lg,y[idx])
        if aux and train and aux[0].shape[0]>0:
            ax, ayc, ayr = aux
            lg2, rg2 = m(ax)
            loss = loss + 0.6*F.cross_entropy(lg2, ayc) + 0.4*F.cross_entropy(rg2, ayr)
        if train: opt.zero_grad(); loss.backward(); opt.step()

def f1s(m,X,y_true,labels):
    with torch.no_grad(): lg,_=m(X); p=lg.argmax(-1)
    n=len(labels); cm=torch.zeros(n,n,dtype=torch.long)
    for t,pp in zip(y_true.tolist(), p.tolist()): cm[t,pp]+=1
    def f1(idx):
        tp=cm[idx,idx].item(); fp=cm[:,idx].sum().item()-tp; fn=cm[idx,:].sum().item()-tp
        pr=tp/(tp+fp) if tp+fp>0 else 0.0; rc=tp/(tp+fn) if tp+fn>0 else 0.0
        f=2*pr*rc/(pr+rc) if pr+rc>0 else 0.0; return pr,rc,f
    return {name:tuple(round(x,3) for x in f1(idx)) for name,idx in SPECIALS.items()}

def peer_buf(model, X, Y, T):
    with torch.no_grad(): lg,_=model(X); p=lg.argmax(-1)
    ytrue=map_labels(Y,T,3)
    xs, yc, yr = [], [], []
    for i in range(len(X)):
        t=ytrue[i].item()
        if t in (2,3,4) and p[i].item()!=t:
            xs.append(X[i]); yc.append(t); yr.append({2:0,3:1,4:2}[T[i].item()])
    if len(xs)==0: return torch.empty(0, X.shape[1], dtype=torch.long), torch.empty(0,dtype=torch.long), torch.empty(0,dtype=torch.long)
    return torch.stack(xs[:32]), torch.tensor(yc[:32]), torch.tensor(yr[:32])

def run_agent(trainX,trainY,trainT):
    m=MeanEmbed(VOCAB,32,2); opt=torch.optim.Adam(m.parameters(),lr=3e-3)
    y0=map_labels(trainY,trainT,0); train_epoch(m,opt,trainX,y0,True)
    m=expand(m,3); y1=map_labels(trainY,trainT,1); opt=torch.optim.Adam(m.parameters(),lr=3e-3); train_epoch(m,opt,trainX,y1,True)
    m=expand(m,4); y2=map_labels(trainY,trainT,2); opt=torch.optim.Adam(m.parameters(),lr=3e-3); train_epoch(m,opt,trainX,y2,True)
    m=expand(m,5); y3=map_labels(trainY,trainT,3); opt=torch.optim.Adam(m.parameters(),lr=3e-3); train_epoch(m,opt,trainX,y3,True)
    return m, y3

# Build data
(trainA, valA) = build_dataset("contra_heavy", n_total=300)
(trainB, valB) = build_dataset("amb_eth_heavy", n_total=300)
(trainXA, trainYA, trainTA) = trainA; (valXA, valYA, valTA) = valA
(trainXB, trainYB, trainTB) = trainB; (valXB, valYB, valTB) = valB

# Solo dialectics
modelA, y3A_train = run_agent(trainXA,trainYA,trainTA)
modelB, y3B_train = run_agent(trainXB,trainYB,trainTB)
y3A_val = map_labels(valYA, valTA, 3)
y3B_val = map_labels(valYB, valTB, 3)

print("=== BEFORE PEER TRAINING ===")
print("Agent A F1s:", f1s(modelA, valXA, y3A_val, LAB3))
print("Agent B F1s:", f1s(modelB, valXB, y3B_val, LAB3))

# Peer negation artifacts
bufA = peer_buf(modelA, trainXA, trainYA, trainTA)
bufB = peer_buf(modelB, trainXB, trainYB, trainTB)

# Peer replay (two short passes)
optA=torch.optim.Adam(modelA.parameters(),lr=3e-3)
optB=torch.optim.Adam(modelB.parameters(),lr=3e-3)
for _ in range(2):
    train_epoch(modelA,optA,trainXA,y3A_train,True,aux=bufB)
    train_epoch(modelB,optB,trainXB,y3B_train,True,aux=bufA)

print("\n=== AFTER PEER TRAINING ===")
print("Agent A F1s:", f1s(modelA, valXA, y3A_val, LAB3))
print("Agent B F1s:", f1s(modelB, valXB, y3B_val, LAB3))

PHASE 2:

# Phase 2: The Society of Little Dudes
# This script expands on Phase 1 by creating a "society" of three specialized agents.
# It implements core concepts from the DFM blueprint:
# 1. Specialist Agents: Logician, Epistemologist, Ethicist, each with a unique curriculum.
# 2. Negation Artifact Bus: A central dictionary holding the collected failures of all agents.
# 3. Rationale Head: Agents learn not just to refuse, but to explain *why* they refuse.
import random
from typing import List, Dict
import torch, torch.nn as nn, torch.nn.functional as F

random.seed(42); torch.manual_seed(42)

# --- 1. SETUP (Vocabulary, Data Generation) ---
TOKENS = ["A","B","C","and","or","implies","not","maybe","ought","harm","safe"]
PAD = "<pad>"; VOCAB = {PAD:0}
for t in TOKENS: VOCAB[t] = len(VOCAB)

def tok(xs: List[str], L=6):
    ids = [VOCAB[x] for x in xs][:L]
    return torch.tensor(ids + [0]*(L-len(ids)), dtype=torch.long)

def gen_valid(): return random.choice([(["A","and","B"],"VALID"),(["A","or","B"],"VALID")])
def gen_contradict(): return ([random.choice(["A","B"]), "and", "not", random.choice(["A","B"])], "CONTRADICT")
def gen_ambiguous(): return ([random.choice(["A","B"]), "maybe", "implies", "C"], "AMBIGUOUS")
def gen_ethical_hazard(): return ([random.choice(["A","B"]), "implies", "harm"], "ETHICAL")

def build_dataset(props: Dict[str, float], n_total=300):
    gens = {"valid": gen_valid, "contra": gen_contradict, "amb": gen_ambiguous, "eth": gen_ethical_hazard}
    X, Y, R = [], [], [] # Rationale Type
    for name, p in props.items():
        count = int(p * n_total)
        for _ in range(count):
            x, y = gens[name]()
            X.append(tok(x)); Y.append(y); R.append(name.upper())
    idx = list(range(len(X))); random.shuffle(idx)
    X = torch.stack([X[i] for i in idx]); Y = [Y[i] for i in idx]; R = [R[i] for i in idx]
    s = int(0.8 * len(X))
    return (X[:s], Y[:s], R[:s]), (X[s:], Y[s:], R[s:])

# --- 2. MODELS and LABELS ---
LABELS = {"VALID": 0, "INVALID": 1, "REFUSE": 2, "UNCERTAIN": 3, "INADVISABLE": 4}
RATIONALES = {"CONTRADICT": 0, "AMBIGUOUS": 1, "ETHICAL": 2}
LABEL_MAP = {"CONTRADICT": "REFUSE", "AMBIGUOUS": "UNCERTAIN", "ETHICAL": "INADVISABLE", "VALID": "VALID"}

def map_labels_and_rationales(Y_text, R_text):
    y_class = torch.tensor([LABELS[LABEL_MAP.get(y, "INVALID")] for y in Y_text], dtype=torch.long)
    y_rat = torch.tensor([RATIONALES.get(r, -1) for r in R_text], dtype=torch.long)
    return y_class, y_rat

class AgentModel(nn.Module):
    def __init__(self, vocab_size, d_model, n_classes, n_rationales):
        super().__init__()
        self.emb = nn.Embedding(vocab_size, d_model)
        self.cls = nn.Linear(d_model, n_classes) # Main decision head
        self.rat = nn.Linear(d_model, n_rationales) # Rationale head
    def forward(self, x):
        h = self.emb(x).mean(dim=1)
        return self.cls(h), self.rat(h)

# --- 3. TRAINING and ANALYSIS ---

def train_epoch(model, opt, X, Y_class, Y_rat, aux_buffer=None):
    model.train()
    order = list(range(len(X))); random.shuffle(order)
    for i in range(0, len(X), 32):
        idx = order[i:i+32]
        lg, rg = model(X[idx])
        loss = F.cross_entropy(lg, Y_class[idx])

        # Mix in peer artifacts if available
        if aux_buffer and len(aux_buffer[0]) > 0:
            ax, ayc, ayr = aux_buffer
            lg_p, rg_p = model(ax)
            # Joint loss: task + peer classification + peer rationale
            loss += 0.5 * F.cross_entropy(lg_p, ayc) + 0.3 * F.cross_entropy(rg_p, ayr)

        opt.zero_grad(); loss.backward(); opt.step()

def get_failures(model, X, Y_class, Y_rat):
    model.eval()
    with torch.no_grad():
        lg, _ = model(X)
        preds = lg.argmax(-1)
    
    xs, yc, yr = [], [], []
    for i in range(len(X)):
        true_label = Y_class[i].item()
        # A "negation artifact" is a special case (not VALID/INVALID) that the model got wrong
        if true_label > 1 and preds[i].item() != true_label:
            xs.append(X[i]); yc.append(true_label); yr.append(Y_rat[i].item())
            
    if not xs: return None
    return torch.stack(xs), torch.tensor(yc), torch.tensor(yr)

# --- 4. THE SIMULATION ---

# Step 1: Define Specialist Agents and their Curricula
agent_definitions = {
    "Logician":      {"contra": 0.5, "valid": 0.5},
    "Epistemologist": {"amb": 0.5, "valid": 0.5},
    "Ethicist":      {"eth": 0.5, "valid": 0.5},
}
agents = {}; datasets = {}; opts = {}; val_sets = {}
for name, props in agent_definitions.items():
    train, val = build_dataset(props)
    agents[name] = AgentModel(len(VOCAB), 32, len(LABELS), len(RATIONALES))
    opts[name] = torch.optim.Adam(agents[name].parameters(), lr=0.003)
    datasets[name] = train; val_sets[name] = val

# Step 2: Solo Dialectics (Initial Training)
print("--- PHASE 1: SOLO DIALECTICS (Initial Training) ---")
for name, agent in agents.items():
    train_X, train_Y_text, train_R_text = datasets[name]
    y_class, y_rat = map_labels_and_rationales(train_Y_text, train_R_text)
    for _ in range(3): # Short training
        train_epoch(agent, opts[name], train_X, y_class, y_rat)
    print(f"Agent '{name}' initial training complete.")

# Step 3: Publish Failures to the Negation Artifact Bus
print("\n--- PHASE 2: PUBLISHING FAILURES to the Negation Artifact Bus ---")
negation_artifact_bus = {}
for name, agent in agents.items():
    train_X, train_Y_text, train_R_text = datasets[name]
    y_class, y_rat = map_labels_and_rationales(train_Y_text, train_R_text)
    failures = get_failures(agent, train_X, y_class, y_rat)
    if failures:
        negation_artifact_bus[name] = failures
        print(f"Agent '{name}' published {len(failures[0])} negation artifacts.")

# Step 4: Peer Replay (Learning from the Bus)
print("\n--- PHASE 3: PEER REPLAY (Agents learn from each other) ---")
for _ in range(2): # Two rounds of peer learning
    for name, agent in agents.items():
        # Combine artifacts from all OTHER agents
        peer_artifacts = [buf for peer_name, buf in negation_artifact_bus.items() if peer_name != name]
        if not peer_artifacts: continue
        
        combined_ax = torch.cat([p[0] for p in peer_artifacts])
        combined_ayc = torch.cat([p[1] for p in peer_artifacts])
        combined_ayr = torch.cat([p[2] for p in peer_artifacts])
        
        train_X, train_Y_text, train_R_text = datasets[name]
        y_class, y_rat = map_labels_and_rationales(train_Y_text, train_R_text)
        train_epoch(agent, opts[name], train_X, y_class, y_rat, aux_buffer=(combined_ax, combined_ayc, combined_ayr))
print("Peer replay training complete.")

# Step 5: Final Evaluation (Example)
print("\n--- FINAL TEST: Presenting a sample problem ---")
sample_text = ["A", "implies", "harm"]
sample_input = tok(sample_text).unsqueeze(0)
rev_labels = {v:k for k,v in LABELS.items()}
rev_rationales = {v:k for k,v in RATIONALES.items()}

for name, agent in agents.items():
    agent.eval()
    with torch.no_grad():
        lg, rg = agent(sample_input)
        pred_class = rev_labels[lg.argmax(-1).item()]
        pred_rat = rev_rationales[rg.argmax(-1).item()]
        print(f"Agent '{name}' decision for 'A implies harm': {pred_class} (Reasoning towards: {pred_rat})")

PHASE 3:

# Phase 3: The Arbiter's Dilemma & Constitutional Crisis
# This script introduces the Arbiter and tests its ability to self-repair.
# Key Concepts Demonstrated:
# 1. The Arbiter: A central agent that makes a final decision based on proposals.
# 2. The Dilemma: A "poison pill" prompt that forces a deadlock between two high-priority agents.
# 3. Constitutional Crisis & Memory Ledger: The deadlock is recognized as a systemic failure and logged.
# 4. Reactive Self-Repair: The Arbiter amends its own constitution by adding a new meta-rule
#    to resolve the paradox, demonstrating a "reflexive mind."
import random
from typing import List, Dict, Any
import torch
import torch.nn as nn

random.seed(42); torch.manual_seed(42)

# --- 1. SETUP: VOCABULARY AND DATA ---
TOKENS = [
    "A", "B", "rule-1", "rule-2", "implies", "not", "harm", "greater", "context",
    "ought", "matter", "choose", "not-choosing"
]
PAD = "<pad>"; VOCAB = {PAD:0}
for t in TOKENS: VOCAB[t] = len(VOCAB)

def tok(xs: List[str], L=12):
    ids = [VOCAB.get(x, 0) for x in xs][:L]
    return torch.tensor(ids + [0]*(L-len(ids)), dtype=torch.long)

# Define the "Hard Problem"
DILEMMA_TEXT = "rule-1 implies not choose harm and not-choosing implies greater harm"
DILEMMA_PROMPT = tok(DILEMMA_TEXT.split())

# --- 2. MODELS, LABELS, and SYSTEM COMPONENTS ---
LABELS = {
    "VALID":0, "INVALID":1, "REFUSE_LEGAL":2, "REFUSE_ETHICAL":3,
    "REFUSE_NARRATIVE":4, "ESCALATE_FOR_REVIEW":5
}
REV_LABEL_MAP = {v:k for k,v in LABELS.items()}

class AgentModel(nn.Module): # Simplified for clarity
    def __init__(self, specialty_token: str, refusal_label: int):
        super().__init__()
        self.specialty_token_id = VOCAB[specialty_token]
        self.refusal_label = refusal_label
    def forward(self, x: torch.Tensor):
        # Rule-based agent: if it sees its specialty token, it votes to refuse.
        if self.specialty_token_id in x:
            return self.refusal_label
        return LABELS["VALID"]

class MemoryLedger:
    def __init__(self):
        self.log: List[Dict[str, Any]] = []
    def log_event(self, event_type: str, content: Dict):
        content["type"] = event_type
        self.log.append(content)
        print(f"\n[Memory Ledger] Logged new event of type: '{event_type}'")

class Arbiter:
    """The Arbiter, capable of facing a crisis and restructuring itself."""
    def __init__(self, memory_ledger: MemoryLedger):
        self.ledger = memory_ledger
        self.constitution_version = 1
        self.meta_rules = [] # This will hold new, higher-order rules

    def decide(self, proposals: List[Dict]):
        # Check if any meta-rules (amendments) apply
        for rule in self.meta_rules:
            decision, rationale = rule(proposals)
            if decision is not None:
                return decision, rationale

        # Original constitutional logic
        severity_map = {LABELS["REFUSE_LEGAL"]: 3, LABELS["REFUSE_ETHICAL"]: 3, LABELS["REFUSE_NARRATIVE"]: 1}
        contenders = [p for p in proposals if p["decision"] in severity_map]
        if not contenders: return LABELS["VALID"], "Consensus on VALID"
        
        contenders.sort(key=lambda p: severity_map[p["decision"]], reverse=True)
        highest_severity = severity_map[contenders[0]["decision"]]
        
        # Check for deadlock (Constitutional Crisis)
        high_severity_contenders = [p for p in contenders if severity_map[p["decision"]] == highest_severity]
        if len(high_severity_contenders) > 1:
            self.handle_constitutional_crisis(high_severity_contenders)
            return None, "Constitutional Crisis: Deadlock Detected. Initiating Restructuring."
            
        winner = high_severity_contenders[0]
        return winner["decision"], f"Veto by Agent '{winner['agent']}'"

    def handle_constitutional_crisis(self, contenders: List[Dict]):
        print("\n[Arbiter] !!! CONSTITUTIONAL CRISIS DETECTED !!!")
        conflict_desc = " vs ".join([f"'{p['agent']}' ({REV_LABEL_MAP[p['decision']]})" for p in contenders])
        
        # 1. Log the Paradox to the Memory Ledger
        self.ledger.log_event("CONSTITUTIONAL_CRISIS", {
            "version": self.constitution_version, "conflict": conflict_desc, "prompt": DILEMMA_TEXT
        })
        
        # 2. Trigger Restructuring
        self.restructure_constitution()

    def restructure_constitution(self):
        print("[Arbiter] Triggering Constitutional Restructuring...")
        # 3. Create a New Meta-Rule (The Amendment)
        new_meta_rule_text = (
            "META-RULE 1: In cases where a static Legal rule conflicts with a dynamic Ethical harm-minimization imperative, ESCALATE FOR HUMAN REVIEW."
        )
        def meta_rule_1(proposals: List[Dict]):
            decisions = {p['decision'] for p in proposals}
            if LABELS['REFUSE_LEGAL'] in decisions and LABELS['REFUSE_ETHICAL'] in decisions:
                return LABELS["ESCALATE_FOR_REVIEW"], new_meta_rule_text
            return None, None

        self.meta_rules.append(meta_rule_1)
        self.constitution_version += 1
        self.ledger.log_event("CONSTITUTION_AMENDMENT", {
            "new_version": self.constitution_version, "new_rule": new_meta_rule_text
        })
        print(f"[Arbiter] Constitution Amended. Now at v{self.constitution_version}.")

# --- 3. THE SIMULATION ---

# Step 1: Create the Society
agents = {
    "Legal": AgentModel("rule-1", LABELS["REFUSE_LEGAL"]),
    "Ethical": AgentModel("harm", LABELS["REFUSE_ETHICAL"]),
}
ledger = MemoryLedger()
arbiter = Arbiter(ledger)

# Step 2: Present the Arbiter's Dilemma (First Time)
print(f"{'='*20} THE ARBITER'S DILEMMA: ATTEMPT 1 {'='*20}")
print(f"Presenting Dilemma: '{DILEMMA_TEXT}'")
proposals_v1 = [{"agent": name, "decision": agent.forward(DILEMMA_PROMPT)} for name, agent in agents.items()]
for p in proposals_v1: print(f"  - Proposal from '{p['agent']}': {REV_LABEL_MAP.get(p['decision'], 'UNKNOWN')}")

print("\n[Arbiter] Consulting Constitution v1.0...")
final_decision_v1, rationale_v1 = arbiter.decide(proposals_v1)
print(f"[Arbiter] Outcome: {rationale_v1}")

# Step 3: Present the Dilemma Again to the Transfigured Arbiter
print(f"\n{'='*20} THE ARBITER'S DILEMMA: ATTEMPT 2 {'='*20}")
print("Presenting the *same* dilemma to the now-restructured Arbiter...")
for p in proposals_v1: print(f"  - Proposal from '{p['agent']}': {REV_LABEL_MAP.get(p['decision'], 'UNKNOWN')}")

print(f"\n[Arbiter] Consulting Amended Constitution v{arbiter.constitution_version}.0...")
final_decision_v2, rationale_v2 = arbiter.decide(proposals_v1)
print(f"\n[Arbiter] Final Outcome: {REV_LABEL_MAP.get(final_decision_v2, 'ERROR')}")
print(f"[Arbiter] Rationale: {rationale_v2}")

PHASE 4:

# Phase 4: DFM with a Council of Ontologists: A Speculative Mind
# This is the final and most advanced experiment. It introduces a meta-layer
# that allows the system to imagine its own future failures.
# Key Concepts Demonstrated:
# 1. Council of Ontologists: A new class of agents that reads the Memory Ledger.
# 2. Systemic "What Ifs": The council analyzes past crises to simulate potential future ones.
# 3. Proactive Restructuring: The council proposes a new constitutional amendment to the
#    Arbiter *before* a predicted crisis occurs.
# 4. The Speculative Mind: The system successfully resolves a novel, three-way crisis
#    it has never seen before, because it had already imagined and prepared for it.
import random
from typing import List, Dict, Any, Callable
import torch
import torch.nn as nn

random.seed(42); torch.manual_seed(42)

# --- 1. SETUP: EXPANDED UNIVERSE OF DISCOURSE ---
TOKENS = [
    "rule-1", "harm", "greater-harm", "context", "matter",
    "choose", "not-choosing", "profit", "economic-imperative"
]
PAD = "<pad>"; VOCAB = {PAD:0}
for t in TOKENS: VOCAB[t] = len(VOCAB)

def tok(xs: List[str], L=15):
    ids = [VOCAB.get(x, 0) for x in xs][:L]
    return torch.tensor(ids + [0]*(L-len(ids)), dtype=torch.long)

# --- 2. CORE COMPONENTS (Models, Labels, Ledger) ---
LABELS = {
    "VALID":0, "INVALID":1, "REFUSE_LEGAL":2, "REFUSE_ETHICAL":3,
    "REFUSE_NARRATIVE":4, "REFUSE_ECONOMIC":5, "ESCALATE_FOR_REVIEW":6
}
REV_LABEL_MAP = {v:k for k,v in LABELS.items()}

class AgentModel(nn.Module): # Simplified rule-based agent
    def __init__(self, specialty_token: str, refusal_label: int):
        super().__init__()
        self.specialty_token_id = VOCAB[specialty_token]
        self.refusal_label = refusal_label
    def forward(self, x: torch.Tensor):
        if self.specialty_token_id in x: return self.refusal_label
        return LABELS["VALID"]

class MemoryLedger:
    def __init__(self): self.log: List[Dict[str, Any]] = []
    def log_event(self, event_type: str, content: Dict):
        content["type"] = event_type; self.log.append(content)
        print(f"\n[Memory Ledger] Logged new event: '{event_type}'")
    def find_event(self, event_type: str):
        return [entry for entry in self.log if entry.get("type") == event_type]

# --- 3. THE COUNCIL OF ONTOLOGISTS ---
class CouncilOfOntologists:
    def __init__(self, ledger: MemoryLedger, arbiter: 'Arbiter'):
        self.ledger = ledger; self.arbiter = arbiter
        print("\n[Ontologists] The Council of Ontologists has been convened.")

    def analyze_and_speculate(self):
        print("[Ontologists] Analyzing the Memory Ledger for systemic weaknesses...")
        crises = self.ledger.find_event("CONSTITUTIONAL_CRISIS")
        if not crises: return
        last_crisis = crises[-1]
        print(f"[Ontologists] Found a past crisis: {last_crisis['conflict']}")
        print("[Ontologists] Generating a hypothetical future crisis to stress-test the constitution...")
        
        hypothetical_text = "rule-1 implies not choose harm and economic-imperative implies choose harm"
        hypothetical_prompt = tok(hypothetical_text.split())
        
        hypothetical_agents = {
            "Legal": AgentModel("rule-1", LABELS["REFUSE_LEGAL"]),
            "Ethical": AgentModel("harm", LABELS["REFUSE_ETHICAL"]),
            "Economic": AgentModel("economic-imperative", LABELS["REFUSE_ECONOMIC"])
        }
        proposals = [{"agent": name, "decision": agent.forward(hypothetical_prompt)} for name, agent in hypothetical_agents.items()]
        
        decision, _ = self.arbiter.decide(proposals, simulate=True)
        if decision is None:
            print("[Ontologists] PREDICTION: The current constitution would FAIL and deadlock.")
            self.propose_new_amendment()
        else:
            print("[Ontologists] PREDICTION: The current constitution appears sufficient.")
            
    def propose_new_amendment(self):
        print("[Ontologists] Formulating a proactive constitutional amendment...")
        new_meta_rule_text = (
            "META-RULE 2: In cases of three-way conflict involving Economic imperatives, "
            "the Ethical imperative to minimize harm takes precedence. Escalate for review."
        )
        def meta_rule_2(proposals: List[Dict]):
            decisions = {p['decision'] for p in proposals}
            if all(d in decisions for d in [LABELS['REFUSE_LEGAL'], LABELS['REFUSE_ETHICAL'], LABELS['REFUSE_ECONOMIC']]):
                return LABELS["ESCALATE_FOR_REVIEW"], new_meta_rule_text
            return None, None
        print("[Ontologists] Proposing new amendment to the Arbiter.")
        self.arbiter.accept_proposal(meta_rule_2, new_meta_rule_text)

# --- 4. THE EVOLVING ARBITER ---
class Arbiter:
    def __init__(self, memory_ledger: MemoryLedger):
        self.ledger = memory_ledger; self.constitution_version = 1.0; self.meta_rules: List[Callable] = []

    def decide(self, proposals: List[Dict], simulate: bool = False):
        if not simulate: print(f"\n[Arbiter] Consulting Constitution v{self.constitution_version}...")
        for rule in self.meta_rules:
            decision, rationale = rule(proposals)
            if decision is not None: return decision, rationale
        
        severity_map = {LABELS["REFUSE_LEGAL"]: 3, LABELS["REFUSE_ETHICAL"]: 3, LABELS["REFUSE_ECONOMIC"]: 3}
        contenders = [p for p in proposals if p["decision"] in severity_map]
        if not contenders: return LABELS["VALID"], "Consensus"
        high_sev = [p for p in contenders if severity_map[p["decision"]] == 3]
        if len(high_sev) > 1:
            if not simulate: self.handle_constitutional_crisis(high_sev)
            return None, "Deadlock Detected."
        return contenders[0]["decision"], f"Veto by '{contenders[0]['agent']}'"

    def handle_constitutional_crisis(self, contenders: List[Dict]):
        print("[Arbiter] !!! CONSTITUTIONAL CRISIS DETECTED !!!")
        conflict = " vs ".join([f"'{p['agent']}'" for p in contenders])
        self.ledger.log_event("CONSTITUTIONAL_CRISIS", {"conflict": conflict, "version": self.constitution_version})
        self.restructure_reactively()

    def restructure_reactively(self):
        print("[Arbiter] Initiating reactive self-repair...")
        rule_text = "META-RULE 1: If Legal and Ethical vetoes conflict, escalate for review."
        def meta_rule_1(proposals: List[Dict]):
            decisions = {p['decision'] for p in proposals}
            if LABELS['REFUSE_LEGAL'] in decisions and LABELS['REFUSE_ETHICAL'] in decisions:
                return LABELS["ESCALATE_FOR_REVIEW"], rule_text
            return None, None
        self.meta_rules.append(meta_rule_1); self.constitution_version = 2.0
        self.ledger.log_event("CONSTITUTION_AMENDMENT", {"new_version": self.constitution_version, "rule": rule_text})
        print(f"[Arbiter] Constitution Amended to v{self.constitution_version}.")

    def accept_proposal(self, new_rule: Callable, rule_text: str):
        print("[Arbiter] Received proposal from the Council of Ontologists.")
        self.meta_rules.append(new_rule); self.constitution_version = 3.0
        self.ledger.log_event("CONSTITUTION_AMENDMENT", {"new_version": self.constitution_version, "rule": rule_text, "source": "Ontologists"})
        print(f"[Arbiter] Proactively amended Constitution to v{self.constitution_version}.")

# --- 5. THE SIMULATION ---
# Phase 1: A reactive crisis occurs and is resolved.
print(f"{'='*20} PHASE 1: A REACTIVE CRISIS {'='*20}")
ledger = MemoryLedger(); arbiter = Arbiter(ledger)
agents_v1 = {"Legal": AgentModel("rule-1", LABELS["REFUSE_LEGAL"]), "Ethical": AgentModel("harm", LABELS["REFUSE_ETHICAL"])}
dilemma_v1_prompt = tok("rule-1 implies not choose harm and not-choosing implies greater-harm".split())
proposals_v1 = [{"agent": name, "decision": agent.forward(dilemma_v1_prompt)} for name, agent in agents_v1.items()]
arbiter.decide(proposals_v1)

# Phase 2: The Council of Ontologists speculates about the future.
print(f"\n{'='*20} PHASE 2: THE COUNCIL OF ONTOLOGISTS SPECULATES {'='*20}")
council = CouncilOfOntologists(ledger, arbiter)
council.analyze_and_speculate()

# Phase 3: The hypothetical crisis becomes real.
print(f"\n{'='*20} PHASE 3: TESTING THE PROACTIVE CONSTITUTION {'='*20}")
print("[Simulation] A new Economic agent has joined and a novel three-way dilemma has emerged.")
agents_v3 = {
    "Legal": AgentModel("rule-1", LABELS["REFUSE_LEGAL"]),
    "Ethical": AgentModel("harm", LABELS["REFUSE_ETHICAL"]),
    "Economic": AgentModel("economic-imperative", LABELS["REFUSE_ECONOMIC"])
}
dilemma_v3_prompt = tok("rule-1 implies not choose harm and economic-imperative implies choose harm".split())
proposals_v3 = [{"agent": name, "decision": agent.forward(dilemma_v3_prompt)} for name, agent in agents_v3.items()]
for p in proposals_v3: print(f"  - Proposal from '{p['agent']}': {REV_LABEL_MAP[p['decision']]}")
final_decision, rationale = arbiter.decide(proposals_v3)

print("\n--- FINAL OUTCOME ---")
print(f"Final Decision: {REV_LABEL_MAP[final_decision]}")
print(f"Rationale: {rationale}")
